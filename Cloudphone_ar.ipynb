{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YghDRHmmymVr5CCt8jvdszu6UuSy5gRh",
      "authorship_tag": "ABX9TyOwCvLirmigCBR2IVhCvpQ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thePegasusai/CloudPhone/blob/main/Cloudphone_ar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbHNZ53Sqbdo"
      },
      "outputs": [],
      "source": [
        "#  mobility phone from the cloud to accomplish milestones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the code to generate text to video in PyTorch\n",
        "To generate a text-to-video in PyTorch, you can use the torchvision package. Here is a basic example:\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
        "\n",
        "# Define video data transforms\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "-We8Gw8YrKMO",
        "outputId": "a36ec1db-1f9b-41ac-b645-6387ef6e8e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-de210272e45e>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    To generate a text-to-video in PyTorch, you can use the torchvision package. Here is a basic example:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a TensorFlow Dataset object for text\n",
        "text_dataset = tf.data.TextLineDataset(text_file_path)\n",
        "\n",
        "# Create a TensorFlow Dataset object for videos\n",
        "video_dataset = tf.data.TFRecordDataset(video_file_path)\n",
        "\n",
        "# Create a Dataset combining both text and video\n",
        "dataset = tf.data.Dataset.zip((text_dataset, video_dataset))\n",
        "\n",
        "# Create a function to process the data\n",
        "def process_data(text, video):\n",
        "    # Process the text\n",
        "    # Process the video\n",
        "    # Return the processed results\n",
        "    return text_processed, video_processed\n",
        "\n",
        "# Apply the processing function to the dataset\n",
        "dataset = dataset.map(process_data)\n",
        "\n",
        "# Generate the text and video embeddings\n",
        "embeddings = model.predict(dataset)\n"
      ],
      "metadata": {
        "id": "XLxW5Biirrdz",
        "outputId": "284a945e-04bc-4d2b-ccbe-0c7e84d18574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-03c7e33543bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a TensorFlow Dataset object for text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a TensorFlow Dataset object for videos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assets"
      ],
      "metadata": {
        "id": "TMc_SWjjq-cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#site assets (gif)\n",
        "=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/0 (2).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/0 (1).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/0 (4).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/0 (5).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/0.gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/1 (1).gif\" height=\"200\" width=\"200\"> \n",
        "  <img src=\"assets/DeepFakes/1.gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/2 (1).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/2 (2).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/2 (3).gif\" height=\"200\" width=\"200\">\n",
        "  <img src=\"assets/DeepFakes/2.gif\" height=\"200\" width=\"200\">\n",
        "    "
      ],
      "metadata": {
        "id": "a2lguQYKcPUw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "697ac0d8-bb7c-4901-d8f3-8b9c26c181bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f2f504aede5b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    =\"200\" width=\"200\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jina"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa0dFefzqwBn",
        "outputId": "725ffb2c-17d6-43e0-c928-4b5af7c9a46f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jina\n",
            "  Downloading jina-3.13.1.tar.gz (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 34.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jcloud>=0.0.35\n",
            "  Downloading jcloud-0.1.6.tar.gz (27 kB)\n",
            "Collecting grpcio-health-checking<1.48.1,>=1.46.0\n",
            "  Downloading grpcio_health_checking-1.47.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from jina) (1.10.2)\n",
            "Collecting docarray>=0.16.4\n",
            "  Downloading docarray-0.20.1.tar.gz (651 kB)\n",
            "\u001b[K     |████████████████████████████████| 651 kB 56.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastapi>=0.76.0\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from jina) (6.0)\n",
            "Collecting opentelemetry-exporter-prometheus>=1.12.0rc1\n",
            "  Downloading opentelemetry_exporter_prometheus-1.12.0rc1-py3-none-any.whl (10 kB)\n",
            "Collecting jina-hubble-sdk>=0.26.12\n",
            "  Downloading jina_hubble_sdk-0.29.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-aiohttp-client>=0.33b0\n",
            "  Downloading opentelemetry_instrumentation_aiohttp_client-0.36b0-py3-none-any.whl (11 kB)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.13.0\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.15.0-py3-none-any.whl (20 kB)\n",
            "Collecting uvicorn[standard]\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from jina) (3.8.3)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting aiostream\n",
            "  Downloading aiostream-0.4.5-py3-none-any.whl (35 kB)\n",
            "Collecting opentelemetry-exporter-otlp>=1.12.0\n",
            "  Downloading opentelemetry_exporter_otlp-1.15.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jina) (2.23.0)\n",
            "Collecting protobuf>=3.20.0\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[K     |████████████████████████████████| 409 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting grpcio<1.48.1,>=1.46.0\n",
            "  Downloading grpcio-1.47.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 62.0 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-sdk>=1.14.0\n",
            "  Downloading opentelemetry_sdk-1.15.0-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-grpc>=0.35b0\n",
            "  Downloading opentelemetry_instrumentation_grpc-0.36b0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from jina) (0.15.0)\n",
            "Collecting docker\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 77.4 MB/s \n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-fastapi>=0.33b0\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.36b0-py3-none-any.whl (11 kB)\n",
            "Collecting uvloop\n",
            "  Downloading uvloop-0.17.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 64.0 MB/s \n",
            "\u001b[?25hCollecting grpcio-reflection<1.48.1,>=1.46.0\n",
            "  Downloading grpcio_reflection-1.47.2-py3-none-any.whl (16 kB)\n",
            "Collecting pathspec\n",
            "  Downloading pathspec-0.10.3-py3-none-any.whl (29 kB)\n",
            "Collecting opentelemetry-api>=1.12.0\n",
            "  Downloading opentelemetry_api-1.15.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from jina) (1.21.6)\n",
            "Collecting websockets\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 72.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from jina) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from jina) (3.8.2)\n",
            "Collecting rich>=12.0.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[K     |████████████████████████████████| 237 kB 72.9 MB/s \n",
            "\u001b[?25hCollecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi>=0.76.0->jina) (4.4.0)\n",
            "Collecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi>=0.76.0->jina) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from grpcio<1.48.1,>=1.46.0->jina) (1.15.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from jcloud>=0.0.35->jina) (2.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.26.12->jina) (5.1.0)\n",
            "Collecting deprecated>=1.2.6\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.8/dist-packages (from opentelemetry-api>=1.12.0->jina) (57.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.12.0->jina) (1.14.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.15.0\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.15.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.8/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.13.0->jina) (1.57.0)\n",
            "Collecting opentelemetry-proto==1.15.0\n",
            "  Downloading opentelemetry_proto-1.15.0-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting backoff<3.0.0,>=1.10.0\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.36b0\n",
            "  Downloading opentelemetry_semantic_conventions-0.36b0-py3-none-any.whl (26 kB)\n",
            "Collecting opentelemetry-instrumentation==0.36b0\n",
            "  Downloading opentelemetry_instrumentation-0.36b0-py3-none-any.whl (24 kB)\n",
            "Collecting opentelemetry-util-http==0.36b0\n",
            "  Downloading opentelemetry_util_http-0.36b0-py3-none-any.whl (6.7 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.36b0\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.36b0-py3-none-any.whl (13 kB)\n",
            "Collecting asgiref~=3.0\n",
            "  Downloading asgiref-3.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->jina) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jina) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jina) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jina) (3.0.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=12.0.0->docarray>=0.16.4->jina) (2.6.1)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting docker\n",
            "  Downloading docker-6.0.0-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 78.2 MB/s \n",
            "\u001b[?25h  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 81.3 MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->jina-hubble-sdk>=0.26.12->jina) (3.11.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn[standard]->jina) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.18.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 70.5 MB/s \n",
            "\u001b[?25hCollecting httptools>=0.5.0\n",
            "  Downloading httptools-0.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
            "\u001b[K     |████████████████████████████████| 427 kB 77.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: jina, docarray, jcloud, python-multipart\n",
            "  Building wheel for jina (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jina: filename=jina-3.13.1-py3-none-any.whl size=317331 sha256=971a39f5797f0da6b46c1c59f46560ff74d0827577d5adc8f54404ddbec19433\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/59/50/6a95786c6ef1ace54635f9c0bf4d2283b35e9eb7a4667ea7f8\n",
            "  Building wheel for docarray (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docarray: filename=docarray-0.20.1-py3-none-any.whl size=712520 sha256=4bb08a69109f5dff7bb41f8c3c99f04a70be5be8b129dc35183fa9f1084ad6c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/7e/a4/cbebd49dcbd2f704da0202b469d883931c24ac78ab365aca23\n",
            "  Building wheel for jcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jcloud: filename=jcloud-0.1.6-py3-none-any.whl size=31539 sha256=a053810ed3558a591217d50eb25841ebb8523d98ef8cc4c54cc053aa6beea7ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/2d/e1/2f81f08a9d0fd2e631315f9a2006cdfd0a0dac62d89822bc1e\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=928afef198f8be9f403a6a0e9baf9174214c64d7f88877477e39040ff35c9bbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built jina docarray jcloud python-multipart\n",
            "Installing collected packages: deprecated, websocket-client, sniffio, protobuf, opentelemetry-semantic-conventions, opentelemetry-api, commonmark, rich, pathspec, opentelemetry-util-http, opentelemetry-sdk, opentelemetry-proto, opentelemetry-instrumentation, h11, grpcio, docker, backoff, asgiref, anyio, websockets, watchfiles, uvloop, uvicorn, starlette, python-dotenv, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, jina-hubble-sdk, httptools, python-multipart, opentelemetry-instrumentation-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-instrumentation-aiohttp-client, opentelemetry-exporter-prometheus, opentelemetry-exporter-otlp, jcloud, grpcio-reflection, grpcio-health-checking, fastapi, docarray, aiostream, aiofiles, jina\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.1\n",
            "    Uninstalling grpcio-1.51.1:\n",
            "      Successfully uninstalled grpcio-1.51.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.47.2 which is incompatible.\u001b[0m\n",
            "Successfully installed aiofiles-22.1.0 aiostream-0.4.5 anyio-3.6.2 asgiref-3.6.0 backoff-2.2.1 commonmark-0.9.1 deprecated-1.2.13 docarray-0.20.1 docker-5.0.3 fastapi-0.88.0 grpcio-1.47.2 grpcio-health-checking-1.47.2 grpcio-reflection-1.47.2 h11-0.14.0 httptools-0.5.0 jcloud-0.1.6 jina-3.13.1 jina-hubble-sdk-0.29.0 opentelemetry-api-1.15.0 opentelemetry-exporter-otlp-1.15.0 opentelemetry-exporter-otlp-proto-grpc-1.15.0 opentelemetry-exporter-otlp-proto-http-1.15.0 opentelemetry-exporter-prometheus-1.12.0rc1 opentelemetry-instrumentation-0.36b0 opentelemetry-instrumentation-aiohttp-client-0.36b0 opentelemetry-instrumentation-asgi-0.36b0 opentelemetry-instrumentation-fastapi-0.36b0 opentelemetry-instrumentation-grpc-0.36b0 opentelemetry-proto-1.15.0 opentelemetry-sdk-1.15.0 opentelemetry-semantic-conventions-0.36b0 opentelemetry-util-http-0.36b0 pathspec-0.10.3 protobuf-4.21.12 python-dotenv-0.21.0 python-multipart-0.0.5 rich-12.6.0 sniffio-1.3.0 starlette-0.22.0 uvicorn-0.20.0 uvloop-0.17.0 watchfiles-0.18.1 websocket-client-1.4.2 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tf-q17TmuvH",
        "outputId": "843545f5-bd62-4ff5-89fb-a31b25c55742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: docarray in /usr/local/lib/python3.8/dist-packages (0.20.1)\n",
            "Requirement already satisfied: jina-hubble-sdk>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from docarray) (0.29.0)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.8/dist-packages (from docarray) (12.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from docarray) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (5.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (3.8.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (6.0)\n",
            "Requirement already satisfied: pathspec in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (0.10.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.8/dist-packages (from jina-hubble-sdk>=0.24.0->docarray) (5.0.3)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=12.0.0->docarray) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=12.0.0->docarray) (4.4.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from rich>=12.0.0->docarray) (0.9.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (6.0.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->jina-hubble-sdk>=0.24.0->docarray) (22.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->jina-hubble-sdk>=0.24.0->docarray) (2.10)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from docker->jina-hubble-sdk>=0.24.0->docarray) (1.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jina-hubble-sdk>=0.24.0->docarray) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jina-hubble-sdk>=0.24.0->docarray) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jina-hubble-sdk>=0.24.0->docarray) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->jina-hubble-sdk>=0.24.0->docarray) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DocArray is a library for nested, unstructured, multimodal data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer multimodal data with a Pythonic API.\n",
        "\n",
        "#🚪 Door to multimodal world: super-expressive data structure for representing complicated/mixed/nested text, image, video, audio, 3D mesh data. The foundation data structure of Jina, CLIP-as-service, DALL·E Flow, DiscoArt etc.\n",
        "\n",
        "#🧑‍🔬 Data science powerhouse: greatly accelerate data scientists’ work on embedding, k-NN matching, querying, visualizing, evaluating via Torch/TensorFlow/ONNX/PaddlePaddle on CPU/GPU.\n",
        "\n",
        "#🚡 Data in transit: optimized for network communication, ready-to-wire at anytime with fast and compressed serialization in Protobuf, bytes, base64, JSON, CSV, DataFrame. Perfect for streaming and out-of-memory data.\n",
        "\n",
        "#🔎 One-stop k-NN: Unified and consistent API for mainstream vector databases that allows nearest neighbor search including Elasticsearch, Redis, AnnLite, Qdrant, Weaviate.\n",
        "\n",
        "#👒 For modern apps: GraphQL support makes your server versatile on request and response; built-in data validation and JSON Schema (OpenAPI) help you build reliable web services.\n",
        "\n",
        "#🐍 Pythonic experience: as easy as a Python list. If you can Python, you can DocArray. Intuitive idioms and type annotation simplify the code you write.\n",
        "\n",
        "#🛸 IDE integration: pretty-print and visualization on Jupyter notebook and Google Colab; comprehensive autocomplete and type hints in PyCharm and VS Code.\n",
        "import docarray\n",
        "docarray.__version__\n",
        "'0.1.0'\n",
        "from docarray import Document, DocumentArray"
      ],
      "metadata": {
        "id": "ErKJSIdKmvBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate text to video using tf.js\n",
        "\n",
        "\n",
        "// Load the model from a local file\n",
        "const model = await tf.loadLayersModel('model.json');\n",
        "\n",
        "// Create a Tensor from the text input\n",
        "const textTensor = tf.tensor2d([[inputText]]);\n",
        "\n",
        "// Create a Tensor from the video input\n",
        "const videoTensor = tf.tensor4d([inputVideo]);\n",
        "\n",
        "// Generate the text and video embeddings\n",
        "const embeddings = model.predict([textTensor, videoTensor]);\n",
        "\n",
        "// Use the embeddings to generate the video\n",
        "const video = generateVideo(embeddings);\n"
      ],
      "metadata": {
        "id": "dA0dNjhnsdTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "R_z_hoaZp17s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive"
      ],
      "metadata": {
        "id": "YCQ6qgoZnH8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t4PpeRm0citc",
        "outputId": "9454734e-ae5e-4ade-9f4f-3fe8f9246ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install notebook   convert "
      ],
      "metadata": {
        "id": "KKGUuOVSavyX",
        "outputId": "bd9b6cbb-d230-4173-d4b6-103b64f791f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (5.7.16)\n",
            "Collecting convert\n",
            "  Downloading convert-0.1.2.tar.gz (561 bytes)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from notebook) (5.3.4)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook) (1.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from notebook) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook) (23.2.1)\n",
            "Requirement already satisfied: tornado<7,>=4.1 in /usr/local/lib/python3.8/dist-packages (from notebook) (6.0.4)\n",
            "Requirement already satisfied: jupyter-client<7.0.0,>=5.2.0 in /usr/local/lib/python3.8/dist-packages (from notebook) (6.1.12)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook) (5.7.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook) (0.13.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook) (0.15.0)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook) (5.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client<7.0.0,>=5.2.0->notebook) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.4.0->notebook) (2.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (5.0.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (2.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook) (5.10.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->jupyter-client<7.0.0,>=5.2.0->notebook) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook) (0.7.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook) (0.5.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel->notebook) (7.9.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->notebook) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook) (0.2.5)\n",
            "Building wheels for collected packages: convert\n",
            "  Building wheel for convert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convert: filename=convert-0.1.2-py3-none-any.whl size=1262 sha256=4afeb69cca6cf18c9e9d2626be26f8f5b3caa7bb528a3cd13e8f2cd2fe40de62\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/0f/f0/67373c182abe68edd86fd4744c06ed8c02910d7e87ea6670ec\n",
            "Successfully built convert\n",
            "Installing collected packages: jedi, convert\n",
            "Successfully installed convert-0.1.2 jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb"
      ],
      "metadata": {
        "id": "Ze3Oxye-F6wm",
        "outputId": "87aad1ee-e305-4447-f801-c576e4ce86c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern '/PATH/TO/YOUR/NOTEBOOKFILE.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            `Exporter` class\n",
            "    Default: 'html'\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the \n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    overwrite base name use for output files.\n",
            "                can only be used when converting one notebook at a time.\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current \n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy \n",
            "            of reveal.js. \n",
            "            For speaker notes to work, this must be a relative path to a local \n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb\n",
            "\n",
            "            which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "\n",
            "            You can specify the export format with `--to`.\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "            can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of \n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d80799e6cbdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    135\u001b[0m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'jupyter nbconvert --to html /PATH/TO/YOUR/NOTEBOOKFILE.ipynb\n' returned non-zero exit status 255."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gumroad "
      ],
      "metadata": {
        "id": "DHa-21A7jnZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gumroad api\n",
        "!pip install pygumroad\n",
        "!git clone https://github.com/opsdisk/pygumroad.git"
      ],
      "metadata": {
        "id": "N2oTWAn_ccmx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dbae7fa-e18b-4d10-b880-b2bff80b2093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygumroad\n",
            "  Downloading pygumroad-0.0.2-py3-none-any.whl (23 kB)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt>=0.9.1\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pygumroad) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pygumroad) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pygumroad) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.24.0->pygumroad) (1.24.3)\n",
            "Installing collected packages: requests, requests-toolbelt, pygumroad\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\n",
            "Successfully installed pygumroad-0.0.2 requests-2.28.1 requests-toolbelt-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pygumroad'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 40 (delta 11), reused 24 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd pygumroad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8WE7WRgnNiu",
        "outputId": "49f22374-7671-4907-a2a9-12e15dcaa979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pygumroad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r requirements.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ex95GVKenIne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dalle 2 pytorch = text to image"
      ],
      "metadata": {
        "id": "c6wA2s2jm-gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dalle2_pytorch\n",
        "\n",
        "import torch\n",
        "from dalle2_pytorch import CLIP\n",
        "\n",
        "clip = CLIP(\n",
        "    dim_text = 512,\n",
        "    dim_image = 512,\n",
        "    dim_latent = 512,\n",
        "    num_text_tokens = 49408,\n",
        "    text_enc_depth = 1,\n",
        "    text_seq_len = 256,\n",
        "    text_heads = 8,\n",
        "    visual_enc_depth = 1,\n",
        "    visual_image_size = 256,\n",
        "    visual_patch_size = 32,\n",
        "    visual_heads = 8,\n",
        "    use_all_token_embeds = True,            # whether to use fine-grained contrastive learning (FILIP)\n",
        "    decoupled_contrastive_learning = True,  # use decoupled contrastive learning (DCL) objective function, removing positive pairs from the denominator of the InfoNCE loss (CLOOB + DCL)\n",
        "    extra_latent_projection = True,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
        "    use_visual_ssl = True,                  # whether to do self supervised learning on images\n",
        "    visual_ssl_type = 'simclr',             # can be either 'simclr' or 'simsiam', depending on using DeCLIP or SLIP\n",
        "    use_mlm = False,                        # use masked language learning (MLM) on text (DeCLIP)\n",
        "    text_ssl_loss_weight = 0.05,            # weight for text MLM loss\n",
        "    image_ssl_loss_weight = 0.05            # weight for image self-supervised learning loss\n",
        ").cuda()\n",
        "\n",
        "# mock data\n",
        "\n",
        "text = torch.randint(0, 49408, (4, 256)).cuda()\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "# train\n",
        "\n",
        "loss = clip(\n",
        "    text,\n",
        "    images,\n",
        "    return_loss = True              # needs to be set to True to return contrastive loss\n",
        ")\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "# do the above with as many texts and images as possible in a loop\n",
        "\n",
        "# train decoder\n",
        "import torch\n",
        "from dalle2_pytorch import Unet, Decoder, CLIP\n",
        "\n",
        "# trained clip from step 1\n",
        "\n",
        "clip = CLIP(\n",
        "    dim_text = 512,\n",
        "    dim_image = 512,\n",
        "    dim_latent = 512,\n",
        "    num_text_tokens = 49408,\n",
        "    text_enc_depth = 1,\n",
        "    text_seq_len = 256,\n",
        "    text_heads = 8,\n",
        "    visual_enc_depth = 1,\n",
        "    visual_image_size = 256,\n",
        "    visual_patch_size = 32,\n",
        "    visual_heads = 8\n",
        ").cuda()\n",
        "\n",
        "# unet for the decoder\n",
        "\n",
        "unet = Unet(\n",
        "    dim = 128,\n",
        "    image_embed_dim = 512,\n",
        "    cond_dim = 128,\n",
        "    channels = 3,\n",
        "    dim_mults=(1, 2, 4, 8)\n",
        ").cuda()\n",
        "\n",
        "# decoder, which contains the unet and clip\n",
        "\n",
        "decoder = Decoder(\n",
        "    unet = unet,\n",
        "    clip = clip,\n",
        "    timesteps = 100,\n",
        "    image_cond_drop_prob = 0.1,\n",
        "    text_cond_drop_prob = 0.5\n",
        ").cuda()\n",
        "\n",
        "# mock images (get a lot of this)\n",
        "\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "# feed images into decoder\n",
        "\n",
        "loss = decoder(images)\n",
        "loss.backward()\n",
        "\n",
        "# do the above for many many many many steps\n",
        "# then it will learn to generate images based on the CLIP image embeddings"
      ],
      "metadata": {
        "id": "ZszL9TiL3rFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from dalle2_pytorch import Unet, Decoder, CLIP\n",
        "\n",
        "# trained clip from step 1\n",
        "\n",
        "clip = CLIP(\n",
        "    dim_text = 512,\n",
        "    dim_image = 512,\n",
        "    dim_latent = 512,\n",
        "    num_text_tokens = 49408,\n",
        "    text_enc_depth = 1,\n",
        "    text_seq_len = 256,\n",
        "    text_heads = 8,\n",
        "    visual_enc_depth = 1,\n",
        "    visual_image_size = 256,\n",
        "    visual_patch_size = 32,\n",
        "    visual_heads = 8\n",
        ").cuda()\n",
        "\n",
        "# unet for the decoder\n",
        "\n",
        "unet = Unet(\n",
        "    dim = 128,\n",
        "    image_embed_dim = 512,\n",
        "    cond_dim = 128,\n",
        "    channels = 3,\n",
        "    dim_mults=(1, 2, 4, 8)\n",
        ").cuda()\n",
        "\n",
        "# decoder, which contains the unet and clip\n",
        "\n",
        "decoder = Decoder(\n",
        "    unet = unet,\n",
        "    clip = clip,\n",
        "    timesteps = 100,\n",
        "    image_cond_drop_prob = 0.1,\n",
        "    text_cond_drop_prob = 0.5\n",
        ").cuda()\n",
        "\n",
        "# mock images (get a lot of this)\n",
        "\n",
        "images = torch.randn(4, 3, 256, 256).cuda()\n",
        "\n",
        "# feed images into decoder\n",
        "\n",
        "loss = decoder(images)\n",
        "loss.backward()\n",
        "\n",
        "# do the above for many many many many steps\n",
        "# then it will learn to generate images based on the CLIP image embeddings"
      ],
      "metadata": {
        "id": "KjATUg5e8KG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}