{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcacA7st7r/Tp+Mr3phNXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26374f121c9147c8b07cca0cf995eff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33ede6c2139f4ac3a8ab48d8dfdb9c3d",
              "IPY_MODEL_3788449694354067992472f6fd289c6e",
              "IPY_MODEL_231917458b2e4e009fdea281e482a9ed"
            ],
            "layout": "IPY_MODEL_be6b5bbb25f64ca5b2b0f6ba851b6936"
          }
        },
        "33ede6c2139f4ac3a8ab48d8dfdb9c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3281509fcfb479d825f7381dfdf5d06",
            "placeholder": "​",
            "style": "IPY_MODEL_c2b31dc6bd89400996bc55d2205876ea",
            "value": "100%"
          }
        },
        "3788449694354067992472f6fd289c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b250d31b1667439f9c5a34c5ff7a62fd",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67e490959b1f4b74b948a5de27c70a61",
            "value": 102530333
          }
        },
        "231917458b2e4e009fdea281e482a9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f5ec543d0c94cdb9a696c61678b568c",
            "placeholder": "​",
            "style": "IPY_MODEL_67105e2b18a14829bc8d69e8e41781cf",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 76.3MB/s]"
          }
        },
        "be6b5bbb25f64ca5b2b0f6ba851b6936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3281509fcfb479d825f7381dfdf5d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b31dc6bd89400996bc55d2205876ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b250d31b1667439f9c5a34c5ff7a62fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e490959b1f4b74b948a5de27c70a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f5ec543d0c94cdb9a696c61678b568c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67105e2b18a14829bc8d69e8e41781cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thePegasusai/CloudPhone/blob/main/VIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Based on](https://github.com/lucidrains/vit-pytorch)"
      ],
      "metadata": {
        "id": "FlnI7acuNHHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8QHTYebLmtQ",
        "outputId": "ee4214e9-4a2d-4b13-d3d5-7e0a3a52b238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-0.40.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from vit-pytorch) (0.14.0+cu116)\n",
            "Collecting einops>=0.6.0\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.8/dist-packages (from vit-pytorch) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.10->vit-pytorch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->vit-pytorch) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->vit-pytorch) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->vit-pytorch) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->vit-pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->vit-pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->vit-pytorch) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->vit-pytorch) (2022.12.7)\n",
            "Installing collected packages: einops, vit-pytorch\n",
            "Successfully installed einops-0.6.0 vit-pytorch-0.40.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "ZTpxa-rQLp3G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch import SimpleViT\n",
        "\n",
        "v = SimpleViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "SkULufHsLp59"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
        "\n",
        "teacher = resnet50(pretrained = True)\n",
        "\n",
        "v = DistillableViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "distiller = DistillWrapper(\n",
        "    student = v,\n",
        "    teacher = teacher,\n",
        "    temperature = 3,           # temperature of distillation\n",
        "    alpha = 0.5,               # trade between main loss and distillation loss\n",
        "    hard = False               # whether to use soft or hard distillation\n",
        ")\n",
        "\n",
        "img = torch.randn(2, 3, 256, 256)\n",
        "labels = torch.randint(0, 1000, (2,))\n",
        "\n",
        "loss = distiller(img, labels)\n",
        "loss.backward()\n",
        "\n",
        "# after lots of training above ...\n",
        "\n",
        "pred = v(img) # (2, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "26374f121c9147c8b07cca0cf995eff9",
            "33ede6c2139f4ac3a8ab48d8dfdb9c3d",
            "3788449694354067992472f6fd289c6e",
            "231917458b2e4e009fdea281e482a9ed",
            "be6b5bbb25f64ca5b2b0f6ba851b6936",
            "c3281509fcfb479d825f7381dfdf5d06",
            "c2b31dc6bd89400996bc55d2205876ea",
            "b250d31b1667439f9c5a34c5ff7a62fd",
            "67e490959b1f4b74b948a5de27c70a61",
            "5f5ec543d0c94cdb9a696c61678b568c",
            "67105e2b18a14829bc8d69e8e41781cf"
          ]
        },
        "id": "3suavm4iLp8C",
        "outputId": "e5358c39-b196-4dd0-d453-ff0c9197e386"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26374f121c9147c8b07cca0cf995eff9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.deepvit import DeepViT\n",
        "\n",
        "v = DeepViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "NvkkVoU2Lp-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.t2t import T2TViT\n",
        "\n",
        "v = T2TViT(\n",
        "    dim = 512,\n",
        "    image_size = 224,\n",
        "    depth = 5,\n",
        "    heads = 8,\n",
        "    mlp_dim = 512,\n",
        "    num_classes = 1000,\n",
        "    t2t_layers = ((7, 4), (3, 2), (3, 2)) # tuples of the kernel size and stride of each consecutive layers of the initial token to token module\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "fJ4jmIYjMHhy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.cct import cct_14\n",
        "\n",
        "cct = cct_14(\n",
        "    img_size = 224,\n",
        "    n_conv_layers = 1,\n",
        "    kernel_size = 7,\n",
        "    stride = 2,\n",
        "    padding = 3,\n",
        "    pooling_kernel_size = 3,\n",
        "    pooling_stride = 2,\n",
        "    pooling_padding = 1,\n",
        "    num_classes = 1000,\n",
        "    positional_embedding = 'learnable', # ['sine', 'learnable', 'none']\n",
        ")"
      ],
      "metadata": {
        "id": "PrNeDXeEMLB_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.cross_vit import CrossViT\n",
        "\n",
        "v = CrossViT(\n",
        "    image_size = 256,\n",
        "    num_classes = 1000,\n",
        "    depth = 4,               # number of multi-scale encoding blocks\n",
        "    sm_dim = 192,            # high res dimension\n",
        "    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n",
        "    sm_enc_depth = 2,        # high res depth\n",
        "    sm_enc_heads = 8,        # high res heads\n",
        "    sm_enc_mlp_dim = 2048,   # high res feedforward dimension\n",
        "    lg_dim = 384,            # low res dimension\n",
        "    lg_patch_size = 64,      # low res patch size\n",
        "    lg_enc_depth = 3,        # low res depth\n",
        "    lg_enc_heads = 8,        # low res heads\n",
        "    lg_enc_mlp_dim = 2048,   # low res feedforward dimensions\n",
        "    cross_attn_depth = 2,    # cross attention rounds\n",
        "    cross_attn_heads = 8,    # cross attention heads\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "pred = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "JSQPUQ8hMP1i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.pit import PiT\n",
        "\n",
        "v = PiT(\n",
        "    image_size = 224,\n",
        "    patch_size = 14,\n",
        "    dim = 256,\n",
        "    num_classes = 1000,\n",
        "    depth = (3, 3, 3),     # list of depths, indicating the number of rounds of each stage before a downsample\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "# forward pass now returns predictions and the attention maps\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "6_5rt9OMMRnV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.levit import LeViT\n",
        "\n",
        "levit = LeViT(\n",
        "    image_size = 224,\n",
        "    num_classes = 1000,\n",
        "    stages = 3,             # number of stages\n",
        "    dim = (256, 384, 512),  # dimensions at each stage\n",
        "    depth = 4,              # transformer of depth 4 at each stage\n",
        "    heads = (4, 6, 8),      # heads at each stage\n",
        "    mlp_mult = 2,\n",
        "    dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "levit(img) # (1, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBC3xIwZMXX0",
        "outputId": "4d98b4c2-05d7-490e-b942-fe123bfb48e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6936e-02, -8.9404e-02,  3.1343e-02,  1.5586e-02, -4.3988e-02,\n",
              "          4.0266e-02, -2.0689e-02, -6.1890e-02, -4.7270e-02, -8.9619e-04,\n",
              "         -2.2118e-02, -1.9455e-02,  9.1915e-02,  1.0613e-02, -1.2791e-02,\n",
              "         -5.3041e-02, -7.3807e-02,  2.0527e-02,  6.7469e-02,  4.0116e-02,\n",
              "         -2.3715e-02,  3.3154e-02, -1.6202e-03, -3.3614e-02,  6.3935e-02,\n",
              "          3.0213e-02,  8.0998e-03,  4.4713e-02,  4.8077e-02,  2.4177e-02,\n",
              "          2.0273e-03, -4.9361e-02, -1.2792e-02,  5.8837e-02,  1.2213e-02,\n",
              "          5.4437e-02,  4.4367e-02, -1.3774e-03,  5.7858e-02, -4.7273e-02,\n",
              "         -3.2769e-02, -9.9394e-03,  2.4854e-02,  1.1276e-02, -2.3140e-02,\n",
              "          6.7542e-02, -2.1380e-02, -1.2203e-02,  1.0582e-02,  7.0746e-02,\n",
              "         -3.0928e-02, -1.9491e-02, -1.7325e-03,  6.5029e-02,  1.7308e-02,\n",
              "          4.4539e-03,  2.8691e-02,  1.6998e-02,  9.7747e-03, -3.3552e-02,\n",
              "         -1.5033e-02, -7.4854e-02,  4.8183e-02, -4.8597e-02,  5.0665e-02,\n",
              "         -4.3094e-02, -2.6422e-02,  5.2012e-02, -1.3827e-02,  8.6505e-04,\n",
              "          2.5875e-02, -4.3163e-02,  4.3576e-02,  1.9264e-02, -8.1622e-02,\n",
              "         -9.4259e-03, -5.8346e-03, -2.4278e-02,  2.7378e-02,  2.2033e-02,\n",
              "         -5.1489e-03, -3.1740e-02,  5.9871e-02,  5.1669e-02, -8.8903e-02,\n",
              "         -2.9362e-02,  2.6813e-03,  5.1848e-02, -1.9804e-02, -5.0384e-02,\n",
              "         -6.3867e-02, -1.8735e-02, -4.3211e-02,  3.5555e-02, -4.5581e-03,\n",
              "         -4.6210e-02, -6.9245e-02,  2.4015e-02,  9.2033e-03,  5.4497e-02,\n",
              "         -6.6255e-03, -3.8278e-03, -5.7012e-02,  6.6959e-02, -3.1577e-02,\n",
              "          4.5069e-02,  1.5405e-03,  1.1134e-02,  2.9160e-02, -2.0882e-02,\n",
              "          3.9755e-02,  1.2609e-01, -9.1572e-02, -1.3531e-02, -2.3406e-02,\n",
              "          2.4280e-02,  2.0158e-02, -4.4712e-02,  3.1706e-02, -1.8388e-02,\n",
              "          7.7852e-02, -1.7665e-02, -2.0413e-02,  9.8460e-03,  6.0035e-03,\n",
              "         -1.5357e-03,  1.1374e-02, -2.2756e-02, -1.9719e-02, -1.7749e-02,\n",
              "         -5.4220e-03,  1.1948e-02, -4.4966e-02,  6.8782e-02, -4.2240e-02,\n",
              "          3.2998e-02, -1.3175e-02,  3.6734e-02, -4.2176e-02,  1.1675e-03,\n",
              "          5.3010e-03,  4.0835e-02,  2.0090e-02,  8.0464e-03, -3.7563e-02,\n",
              "          5.3284e-03, -3.5614e-02, -1.9516e-02,  3.9831e-02, -3.4675e-02,\n",
              "         -2.5067e-02, -2.4531e-02,  2.0778e-03, -9.6378e-03, -1.6358e-02,\n",
              "         -4.9497e-02, -6.2962e-02, -3.4154e-02, -1.0986e-02, -6.6798e-04,\n",
              "         -5.8092e-04,  1.2112e-02,  5.6064e-03,  5.8431e-03, -1.2690e-02,\n",
              "         -1.1964e-02, -4.2720e-02, -2.3452e-02, -1.8596e-02, -5.4190e-03,\n",
              "         -4.9662e-03,  3.6295e-02, -3.2436e-02, -7.3972e-02, -4.8091e-03,\n",
              "          1.8377e-02,  8.8424e-03,  5.2739e-03,  1.6059e-02, -6.6639e-02,\n",
              "         -3.0441e-02, -2.8021e-02, -1.0023e-02, -4.6418e-02, -1.3165e-02,\n",
              "          6.2282e-02,  2.7750e-02,  3.5765e-03,  3.8788e-03,  1.6999e-02,\n",
              "         -1.2200e-02,  7.4619e-02,  3.5348e-02,  5.4969e-02, -4.7995e-02,\n",
              "         -2.1580e-02, -3.3581e-02, -2.6355e-02,  2.2250e-02,  6.0273e-02,\n",
              "         -3.4600e-02, -2.7094e-02, -4.8962e-02,  2.8016e-02,  2.0137e-03,\n",
              "          1.5300e-02,  2.4319e-02, -4.4101e-02,  2.5422e-02, -8.7626e-03,\n",
              "         -3.6367e-02,  2.0336e-02,  1.6777e-02, -5.6607e-02, -2.3902e-02,\n",
              "          3.7949e-02,  8.8454e-02,  1.3924e-02,  6.3672e-02,  4.5194e-02,\n",
              "         -5.1737e-02, -5.0883e-02, -2.2041e-02, -8.3261e-02, -1.4511e-02,\n",
              "         -4.5060e-02, -4.0308e-02, -1.9428e-02, -4.0462e-03, -1.0170e-02,\n",
              "         -1.2247e-02, -7.0891e-02,  1.3024e-02, -3.3543e-02, -4.9027e-02,\n",
              "          2.4820e-02,  4.8502e-02,  5.5805e-02, -6.3324e-02,  5.7063e-03,\n",
              "          4.8378e-02,  4.9480e-02, -3.2753e-02, -1.3664e-02,  4.1192e-02,\n",
              "          4.1899e-02, -3.8059e-02,  3.2191e-02,  1.9315e-02,  1.2580e-02,\n",
              "          1.7296e-02,  1.6536e-02, -5.7915e-05, -1.3990e-02, -2.4291e-02,\n",
              "         -7.4521e-03,  6.7919e-02, -5.1711e-02,  2.5855e-02, -9.5505e-02,\n",
              "          1.6796e-02, -3.3938e-02,  1.5907e-02, -8.4112e-03, -4.6530e-02,\n",
              "         -7.4834e-02, -4.5503e-02, -1.2186e-02,  1.5309e-02,  1.5620e-02,\n",
              "          2.1549e-02,  5.5488e-02, -7.5730e-02,  4.6287e-02, -1.6900e-02,\n",
              "         -1.4246e-03, -2.0005e-02,  1.1396e-02,  6.8684e-02,  2.1764e-03,\n",
              "         -5.3090e-03,  3.6372e-02,  6.9597e-03, -1.0120e-02,  6.3505e-02,\n",
              "          6.1813e-02,  4.4580e-02,  8.1684e-03, -1.8873e-02, -2.2085e-02,\n",
              "         -3.0039e-02,  2.2377e-02,  8.5264e-03,  1.8565e-02,  3.9920e-02,\n",
              "         -4.5132e-02, -4.1401e-02,  1.7030e-02, -6.7264e-02,  7.1638e-02,\n",
              "         -4.8034e-02,  2.1698e-02,  1.2574e-02, -7.4419e-02,  3.1780e-02,\n",
              "         -1.1435e-02, -1.6770e-02, -4.8117e-02,  1.7342e-02, -1.9737e-02,\n",
              "          8.9819e-03, -2.0365e-02, -2.0703e-02,  5.1386e-03,  2.8605e-02,\n",
              "         -4.1005e-02, -4.5931e-03,  3.0392e-02,  6.2175e-05, -4.5307e-03,\n",
              "          2.9312e-03, -3.8442e-02, -1.7997e-02, -1.3540e-02, -2.9308e-03,\n",
              "          2.4778e-02, -1.1461e-02,  1.1113e-02, -1.4122e-02,  2.3459e-02,\n",
              "         -2.0292e-02, -3.4898e-02, -4.6194e-02,  1.0873e-02,  4.8782e-03,\n",
              "         -2.8247e-02, -1.0437e-02, -8.8470e-03,  2.6654e-02,  1.1696e-02,\n",
              "          3.8217e-02, -2.0170e-03, -7.4071e-03, -4.1113e-02,  1.0568e-02,\n",
              "          5.2934e-03,  4.4861e-02,  6.4660e-02,  4.1190e-02,  5.0290e-02,\n",
              "         -1.8720e-02,  1.7021e-02,  7.2553e-03, -3.3577e-02, -3.1005e-03,\n",
              "         -3.4065e-02, -2.9758e-02, -2.1755e-02,  2.3809e-02, -1.5722e-02,\n",
              "         -1.3579e-03,  1.0705e-03, -4.5920e-02, -5.8346e-03,  2.7967e-02,\n",
              "         -6.2277e-02,  5.0379e-02, -1.4584e-02, -6.5539e-02, -1.3149e-02,\n",
              "         -1.0116e-02,  2.0247e-02, -9.4128e-02, -6.7636e-03,  5.4516e-02,\n",
              "         -4.1265e-03, -3.1604e-02,  3.0831e-02, -4.3440e-02,  4.7286e-02,\n",
              "         -5.5641e-02,  3.6725e-02, -3.1260e-02,  6.2124e-02,  4.0759e-02,\n",
              "         -7.1398e-02,  3.8237e-02,  4.8551e-02,  3.0687e-02,  8.7679e-03,\n",
              "         -9.1806e-03,  5.0960e-02,  2.2759e-03, -1.9701e-02, -2.0340e-03,\n",
              "         -1.3903e-02,  6.8924e-03, -1.0247e-02, -2.2036e-03, -1.9985e-02,\n",
              "          2.1368e-04, -4.2928e-02, -6.0910e-02, -1.0593e-02, -1.3183e-02,\n",
              "          1.0813e-02,  6.7035e-02,  8.2009e-03, -1.1029e-02,  7.7750e-03,\n",
              "          6.2296e-02, -7.0899e-02,  2.7714e-04,  2.0792e-02, -8.7196e-03,\n",
              "         -3.6552e-02, -3.4404e-02,  5.4844e-02,  4.6765e-02, -4.1616e-02,\n",
              "         -1.3809e-02, -4.8191e-03,  3.4800e-03, -3.5173e-02,  2.8084e-02,\n",
              "          7.3566e-02, -3.3346e-02,  4.5922e-03, -1.4791e-02,  6.4009e-02,\n",
              "         -3.6707e-02, -5.9955e-02,  4.8636e-02, -3.6908e-02,  2.5244e-02,\n",
              "          8.7044e-03,  6.3277e-03,  3.4543e-02, -1.7613e-02,  3.2123e-02,\n",
              "          3.9048e-02,  1.4562e-02,  1.1118e-02,  2.4773e-02, -4.9584e-02,\n",
              "         -2.0888e-02, -4.1963e-02, -1.8121e-02,  5.6185e-02,  1.9750e-02,\n",
              "          1.8086e-02,  6.0828e-02, -3.2058e-02,  7.8363e-04,  5.8648e-02,\n",
              "         -5.6147e-02,  3.0424e-02, -1.6990e-02,  3.9142e-02, -5.3787e-03,\n",
              "         -3.4596e-02,  1.5091e-02, -3.4629e-02, -7.2977e-03, -2.7847e-02,\n",
              "          4.7750e-02,  2.9542e-02,  3.4609e-02, -2.3804e-02, -8.4957e-03,\n",
              "         -5.0507e-03,  9.7484e-03,  4.0778e-02,  2.8810e-02,  6.2656e-02,\n",
              "         -2.0444e-03,  1.6321e-02,  4.0811e-02, -6.4834e-02,  3.9304e-02,\n",
              "          7.2619e-02, -6.4171e-02, -4.9669e-03,  2.3129e-02, -5.0692e-02,\n",
              "          1.1537e-02, -1.3535e-02, -4.3183e-02, -2.9490e-02, -2.2925e-03,\n",
              "         -8.4675e-03, -6.9956e-03,  3.7539e-03, -3.4032e-02, -3.8028e-02,\n",
              "          6.3509e-02, -3.5877e-02,  9.3832e-03,  1.3387e-02,  2.3395e-02,\n",
              "         -3.2189e-02,  3.6888e-02,  1.6383e-02,  6.2159e-02, -4.7071e-02,\n",
              "          6.4690e-02, -1.6702e-03,  4.9910e-02, -2.4310e-02, -1.8678e-02,\n",
              "         -4.8493e-02, -3.5728e-02,  2.4763e-02,  2.2783e-02, -5.3135e-02,\n",
              "          3.7892e-02,  1.5230e-02,  3.1188e-02,  2.7467e-02,  4.8999e-02,\n",
              "         -4.1937e-02, -3.3508e-02, -2.1708e-04,  6.7404e-02, -3.4070e-02,\n",
              "         -1.6638e-02,  3.0031e-02, -2.3700e-02,  3.1994e-03,  2.3557e-02,\n",
              "          3.9728e-03, -1.7888e-02,  3.2759e-02, -5.2790e-02, -1.0974e-03,\n",
              "          1.8454e-02,  4.9117e-02, -2.7866e-02, -3.1787e-02,  2.0205e-02,\n",
              "         -7.0939e-03, -1.1792e-04, -3.3646e-03, -1.7822e-02,  1.8052e-02,\n",
              "         -2.0033e-02, -5.3603e-02, -5.1257e-02, -5.4127e-02, -3.8242e-02,\n",
              "          3.3936e-02, -1.4439e-02,  1.2054e-02,  5.1075e-03, -5.6642e-02,\n",
              "          3.9673e-02, -6.4047e-02, -8.2573e-03,  7.1672e-03, -1.9809e-03,\n",
              "          2.7218e-02, -1.2365e-02,  8.1892e-03, -2.7251e-03, -1.6645e-02,\n",
              "         -1.3194e-02,  5.1257e-03,  8.7144e-03,  6.4179e-02,  1.3825e-02,\n",
              "         -3.1781e-02, -3.9103e-02, -4.5213e-02,  2.2798e-02, -6.9486e-02,\n",
              "         -4.3172e-02,  3.0277e-02,  7.9750e-03,  5.5065e-02,  1.1061e-02,\n",
              "          4.4802e-03, -7.8594e-02, -2.6324e-02, -1.2993e-02, -3.7567e-02,\n",
              "          3.1058e-02, -4.7742e-02, -5.1576e-02,  1.6776e-02, -1.5679e-02,\n",
              "          3.3569e-02,  1.4797e-02,  2.9679e-02, -5.1511e-02,  5.2301e-02,\n",
              "          5.4005e-02,  8.5290e-03,  7.5539e-03,  7.8054e-03, -4.4143e-02,\n",
              "         -2.6990e-02,  2.9579e-02,  2.5036e-02, -5.3677e-03, -4.7062e-02,\n",
              "         -2.7326e-02, -8.0545e-02,  2.2468e-02, -5.8305e-02,  1.7464e-02,\n",
              "         -5.1038e-02, -8.9094e-03, -5.4195e-02,  1.3418e-02, -5.2274e-03,\n",
              "         -2.3646e-03,  5.7487e-03,  5.1326e-02, -3.0404e-02, -2.1743e-02,\n",
              "         -3.7018e-02, -2.7248e-02,  2.0793e-02, -5.2472e-02,  3.2284e-02,\n",
              "         -4.3214e-02,  3.0558e-02, -7.0082e-03, -9.5248e-02,  2.4675e-02,\n",
              "         -2.3924e-02,  5.1083e-02, -2.7159e-03, -2.1019e-02,  3.8487e-02,\n",
              "          6.3133e-02, -7.8216e-02,  4.7384e-02, -8.0927e-02,  7.1138e-02,\n",
              "          5.1854e-02, -7.6645e-03, -2.3782e-02,  2.1013e-02,  1.4455e-02,\n",
              "          2.2555e-02, -5.0663e-03,  2.6794e-02, -1.4047e-02, -1.7186e-02,\n",
              "         -2.2621e-02, -1.2635e-02, -8.0063e-02, -3.5626e-02,  4.4913e-02,\n",
              "         -2.7784e-02, -2.6740e-02, -2.3864e-03, -5.5926e-03, -6.6624e-03,\n",
              "         -3.5191e-02, -4.4322e-02,  2.8621e-02,  2.4663e-02, -2.9617e-02,\n",
              "          1.9680e-02,  5.6395e-02,  1.1463e-02,  1.6901e-02,  1.3188e-02,\n",
              "          6.2114e-02,  3.1395e-03,  2.4903e-02,  4.4254e-02,  3.6011e-02,\n",
              "         -9.6367e-03, -3.3680e-02,  2.7454e-03, -7.5956e-02, -5.5803e-02,\n",
              "          5.3265e-02, -9.4175e-04,  3.8605e-04, -2.2982e-03, -6.2155e-03,\n",
              "          4.0536e-03, -2.3177e-02, -1.3041e-03, -1.6995e-02, -5.5847e-02,\n",
              "         -7.0361e-03, -1.6789e-02,  8.9421e-02,  4.1220e-02,  2.9048e-02,\n",
              "         -3.8158e-02,  1.1177e-02, -7.2151e-02, -4.0405e-02,  1.9402e-02,\n",
              "          7.8921e-02, -9.9926e-03,  2.8750e-02,  1.3068e-02, -5.5646e-02,\n",
              "         -3.6217e-02, -2.5903e-02,  2.3056e-02,  7.1724e-03,  5.0192e-02,\n",
              "         -1.5648e-02,  6.0738e-04, -4.5945e-03,  3.6364e-02,  8.4016e-03,\n",
              "          4.4066e-02, -2.1993e-02, -3.5548e-02,  4.0247e-02, -4.4649e-02,\n",
              "          2.7659e-02,  1.7945e-02, -3.8680e-02, -3.5460e-02,  4.0926e-03,\n",
              "         -9.9533e-03,  1.1084e-02,  6.8755e-02, -4.1045e-02, -1.6412e-02,\n",
              "         -4.6859e-02, -1.2314e-02, -3.3119e-02, -5.2347e-03, -2.0396e-02,\n",
              "          4.6317e-02,  3.4621e-02,  2.2754e-02, -5.2678e-02,  6.7515e-03,\n",
              "          1.5782e-02, -4.7024e-02, -6.1252e-03,  6.3042e-02, -2.7347e-02,\n",
              "          5.4615e-02,  4.6974e-02,  7.5795e-02,  2.6871e-02, -8.1922e-02,\n",
              "         -5.6283e-03,  3.9511e-02,  4.4146e-02,  3.3587e-03, -4.6463e-02,\n",
              "         -1.2853e-03,  5.7804e-02,  7.1837e-02, -5.0073e-02,  2.3019e-02,\n",
              "          5.8853e-02, -1.7894e-02, -1.0738e-02, -4.0531e-02, -4.8588e-02,\n",
              "         -2.0680e-02,  5.3523e-02, -3.8417e-02, -6.1214e-02, -1.1549e-02,\n",
              "          1.4189e-02,  1.8983e-02, -2.0232e-02,  3.1413e-02, -4.5182e-02,\n",
              "          3.3978e-02, -5.8022e-02, -2.6346e-03, -7.6351e-03, -5.0106e-02,\n",
              "         -1.3177e-02,  2.0255e-02, -4.0145e-03, -4.6253e-02,  7.5058e-03,\n",
              "         -2.3460e-02, -1.0733e-02, -6.6284e-02,  1.1584e-02, -5.7203e-02,\n",
              "         -9.0622e-03,  5.8631e-02, -3.2959e-02,  2.0758e-02,  3.5689e-02,\n",
              "          3.4941e-02,  3.6096e-02,  1.7988e-02, -3.2511e-02, -2.2825e-02,\n",
              "         -7.2908e-03,  3.1865e-02, -9.1612e-03, -1.1010e-02, -4.8393e-02,\n",
              "          3.2922e-02,  1.4344e-02, -3.2334e-02,  2.6356e-02,  1.9269e-02,\n",
              "         -5.5099e-03, -5.6800e-02,  1.7417e-02, -3.5348e-02,  2.4608e-02,\n",
              "         -3.5154e-03, -3.3874e-02, -3.2201e-02,  1.6294e-02,  3.4987e-02,\n",
              "         -7.4978e-03, -6.5859e-02,  3.0220e-03, -4.1350e-03, -1.3552e-02,\n",
              "         -3.9908e-02,  2.9187e-02, -1.0853e-02,  2.4781e-02, -1.2027e-02,\n",
              "         -2.3887e-02,  4.6520e-02,  1.1844e-02, -2.4563e-02, -4.9495e-02,\n",
              "         -2.4356e-03, -3.6177e-02,  1.6916e-02, -6.5941e-02, -1.3110e-02,\n",
              "          8.2512e-03,  7.7446e-02,  1.0683e-02, -2.1811e-02, -3.2972e-02,\n",
              "         -1.8772e-02,  1.3727e-02,  2.5801e-02, -4.7268e-03,  6.2267e-02,\n",
              "          4.2299e-03,  5.6888e-02,  2.5765e-02, -2.3940e-02,  6.2371e-03,\n",
              "          3.3727e-02,  2.5776e-02, -5.0173e-02, -1.2970e-02, -3.1894e-02,\n",
              "          1.4325e-02, -3.7535e-02, -5.6269e-02,  3.3135e-02, -1.9163e-02,\n",
              "         -3.1553e-02, -2.8087e-02,  5.6891e-02, -2.0249e-02,  3.1232e-02,\n",
              "          1.0993e-02,  4.2778e-02, -1.9216e-02,  9.3639e-04,  4.5898e-02,\n",
              "         -3.0107e-02, -3.7483e-02, -2.5520e-02, -1.3959e-02, -1.0504e-03,\n",
              "         -8.6474e-03,  3.7049e-03,  1.1402e-02, -6.3145e-02,  3.5408e-02,\n",
              "          1.3639e-02, -1.9488e-02, -6.2788e-02, -2.2520e-02, -1.4808e-02,\n",
              "          1.1443e-02, -5.8332e-02,  4.0258e-02,  1.8699e-02, -7.8924e-03,\n",
              "         -9.5102e-03, -1.4296e-02,  5.5427e-02,  5.4777e-02,  1.1292e-01,\n",
              "          5.1885e-02,  7.4731e-03,  2.6514e-03,  1.4177e-02, -3.1929e-03,\n",
              "         -7.4645e-02, -4.5927e-02, -2.7165e-02,  2.1195e-02,  1.0819e-02,\n",
              "          2.8594e-02, -1.5078e-03,  1.2338e-02, -4.4660e-03,  5.3830e-02,\n",
              "          2.6072e-02, -9.4505e-04,  2.3960e-02,  6.3395e-04,  6.6862e-02,\n",
              "          9.0301e-04,  1.7401e-02,  4.7558e-02, -2.4014e-02,  4.7749e-04,\n",
              "          2.2082e-02, -8.3136e-03,  4.4530e-02, -5.6801e-02,  1.0217e-02,\n",
              "          7.4436e-02,  6.6211e-02,  4.7964e-02,  1.7851e-02, -4.4911e-02,\n",
              "         -3.7152e-02,  2.0788e-02,  5.4686e-02,  4.5166e-02, -2.5313e-02,\n",
              "          2.8085e-02, -9.7182e-03,  2.0298e-02,  5.1122e-02, -5.0088e-03,\n",
              "          2.1981e-03,  8.0107e-03, -5.2926e-03, -5.1816e-03,  1.1092e-03,\n",
              "          1.9818e-02, -8.2663e-03,  1.2850e-03,  6.2743e-02, -5.7328e-02,\n",
              "         -5.4489e-02,  4.3688e-02, -4.1849e-03,  2.5089e-02,  2.7919e-02,\n",
              "         -3.8467e-02,  7.1870e-02,  2.7440e-02, -2.6206e-03, -7.4138e-04,\n",
              "          5.2877e-02,  8.7510e-02,  7.5749e-02,  6.3563e-02, -1.1083e-03,\n",
              "         -6.1434e-03,  1.0857e-02,  4.5510e-02,  8.0359e-03, -2.6092e-02,\n",
              "         -1.7490e-02,  1.8120e-02, -3.8821e-02,  4.1210e-02, -1.6657e-03,\n",
              "          3.0284e-02, -2.4793e-02, -4.6520e-02,  1.5835e-02, -4.4017e-02,\n",
              "         -5.1427e-02,  2.9706e-03, -2.1672e-02, -3.0666e-02,  4.0889e-02,\n",
              "          4.4236e-02, -6.1673e-02, -2.5060e-03,  2.7488e-03, -8.1239e-03,\n",
              "          6.7861e-02, -1.5940e-02, -4.8648e-02, -5.9942e-03,  1.5365e-02]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.cvt import CvT\n",
        "\n",
        "v = CvT(\n",
        "    num_classes = 1000,\n",
        "    s1_emb_dim = 64,        # stage 1 - dimension\n",
        "    s1_emb_kernel = 7,      # stage 1 - conv kernel\n",
        "    s1_emb_stride = 4,      # stage 1 - conv stride\n",
        "    s1_proj_kernel = 3,     # stage 1 - attention ds-conv kernel size\n",
        "    s1_kv_proj_stride = 2,  # stage 1 - attention key / value projection stride\n",
        "    s1_heads = 1,           # stage 1 - heads\n",
        "    s1_depth = 1,           # stage 1 - depth\n",
        "    s1_mlp_mult = 4,        # stage 1 - feedforward expansion factor\n",
        "    s2_emb_dim = 192,       # stage 2 - (same as above)\n",
        "    s2_emb_kernel = 3,\n",
        "    s2_emb_stride = 2,\n",
        "    s2_proj_kernel = 3,\n",
        "    s2_kv_proj_stride = 2,\n",
        "    s2_heads = 3,\n",
        "    s2_depth = 2,\n",
        "    s2_mlp_mult = 4,\n",
        "    s3_emb_dim = 384,       # stage 3 - (same as above)\n",
        "    s3_emb_kernel = 3,\n",
        "    s3_emb_stride = 2,\n",
        "    s3_proj_kernel = 3,\n",
        "    s3_kv_proj_stride = 2,\n",
        "    s3_heads = 4,\n",
        "    s3_depth = 10,\n",
        "    s3_mlp_mult = 4,\n",
        "    dropout = 0.\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "pred = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "NyzxGjidMasT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.twins_svt import TwinsSVT\n",
        "\n",
        "model = TwinsSVT(\n",
        "    num_classes = 1000,       # number of output classes\n",
        "    s1_emb_dim = 64,          # stage 1 - patch embedding projected dimension\n",
        "    s1_patch_size = 4,        # stage 1 - patch size for patch embedding\n",
        "    s1_local_patch_size = 7,  # stage 1 - patch size for local attention\n",
        "    s1_global_k = 7,          # stage 1 - global attention key / value reduction factor, defaults to 7 as specified in paper\n",
        "    s1_depth = 1,             # stage 1 - number of transformer blocks (local attn -> ff -> global attn -> ff)\n",
        "    s2_emb_dim = 128,         # stage 2 (same as above)\n",
        "    s2_patch_size = 2,\n",
        "    s2_local_patch_size = 7,\n",
        "    s2_global_k = 7,\n",
        "    s2_depth = 1,\n",
        "    s3_emb_dim = 256,         # stage 3 (same as above)\n",
        "    s3_patch_size = 2,\n",
        "    s3_local_patch_size = 7,\n",
        "    s3_global_k = 7,\n",
        "    s3_depth = 5,\n",
        "    s4_emb_dim = 512,         # stage 4 (same as above)\n",
        "    s4_patch_size = 2,\n",
        "    s4_local_patch_size = 7,\n",
        "    s4_global_k = 7,\n",
        "    s4_depth = 4,\n",
        "    peg_kernel_size = 3,      # positional encoding generator kernel size\n",
        "    dropout = 0.              # dropout\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "pred = model(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "tVLyh1sKMeAz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.regionvit import RegionViT\n",
        "\n",
        "model = RegionViT(\n",
        "    dim = (64, 128, 256, 512),      # tuple of size 4, indicating dimension at each stage\n",
        "    depth = (2, 2, 8, 2),           # depth of the region to local transformer at each stage\n",
        "    window_size = 7,                # window size, which should be either 7 or 14\n",
        "    num_classes = 1000,             # number of output classes\n",
        "    tokenize_local_3_conv = False,  # whether to use a 3 layer convolution to encode the local tokens from the image. the paper uses this for the smaller models, but uses only 1 conv (set to False) for the larger models\n",
        "    use_peg = False,                # whether to use positional generating module. they used this for object detection for a boost in performance\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "pred = model(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "kxc6E1kVMgau"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.crossformer import CrossFormer\n",
        "\n",
        "model = CrossFormer(\n",
        "    num_classes = 1000,                # number of output classes\n",
        "    dim = (64, 128, 256, 512),         # dimension at each stage\n",
        "    depth = (2, 2, 8, 2),              # depth of transformer at each stage\n",
        "    global_window_size = (8, 4, 2, 1), # global window sizes at each stage\n",
        "    local_window_size = 7,             # local window size (can be customized for each stage, but in paper, held constant at 7 for all stages)\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "pred = model(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "BnzgHd5yMjez"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.scalable_vit import ScalableViT\n",
        "\n",
        "model = ScalableViT(\n",
        "    num_classes = 1000,\n",
        "    dim = 64,                               # starting model dimension. at every stage, dimension is doubled\n",
        "    heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n",
        "    depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n",
        "    ssa_dim_key = (40, 40, 40, 32),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n",
        "    reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n",
        "    window_size = (64, 32, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n",
        "    dropout = 0.1,                          # attention and feedforward dropout\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = model(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "4Xv516lnMm03"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.sep_vit import SepViT\n",
        "\n",
        "v = SepViT(\n",
        "    num_classes = 1000,\n",
        "    dim = 32,               # dimensions of first stage, which doubles every stage (32, 64, 128, 256) for SepViT-Lite\n",
        "    dim_head = 32,          # attention head dimension\n",
        "    heads = (1, 2, 4, 8),   # number of heads per stage\n",
        "    depth = (1, 2, 6, 2),   # number of transformer blocks per stage\n",
        "    window_size = 7,        # window size of DSS Attention block\n",
        "    dropout = 0.1           # dropout\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "15zh3bcbMqZQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.max_vit import MaxViT\n",
        "\n",
        "v = MaxViT(\n",
        "    num_classes = 1000,\n",
        "    dim_conv_stem = 64,               # dimension of the convolutional stem, would default to dimension of first layer if not specified\n",
        "    dim = 96,                         # dimension of first layer, doubles every layer\n",
        "    dim_head = 32,                    # dimension of attention heads, kept at 32 in paper\n",
        "    depth = (2, 2, 5, 2),             # number of MaxViT blocks per stage, which consists of MBConv, block-like attention, grid-like attention\n",
        "    window_size = 7,                  # window size for block and grids\n",
        "    mbconv_expansion_rate = 4,        # expansion rate of MBConv\n",
        "    mbconv_shrinkage_rate = 0.25,     # shrinkage rate of squeeze-excitation in MBConv\n",
        "    dropout = 0.1                     # dropout\n",
        ")\n",
        "\n",
        "img = torch.randn(2, 3, 224, 224)\n",
        "\n",
        "preds = v(img) # (2, 1000)"
      ],
      "metadata": {
        "id": "ixbVPe6cMvdt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch.nest import NesT\n",
        "\n",
        "nest = NesT(\n",
        "    image_size = 224,\n",
        "    patch_size = 4,\n",
        "    dim = 96,\n",
        "    heads = 3,\n",
        "    num_hierarchies = 3,        # number of hierarchies\n",
        "    block_repeats = (2, 2, 8),  # the number of transformer blocks at each hierarchy, starting from the bottom\n",
        "    num_classes = 1000\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "pred = nest(img) # (1, 1000)"
      ],
      "metadata": {
        "id": "4PTw0KWyMxVx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "from vit_pytorch.simmim import SimMIM\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "mim = SimMIM(\n",
        "    encoder = v,\n",
        "    masking_ratio = 0.5  # they found 50% to yield the best results\n",
        ")\n",
        "\n",
        "images = torch.randn(8, 3, 256, 256)\n",
        "\n",
        "loss = mim(images)\n",
        "loss.backward()\n",
        "\n",
        "# that's all!\n",
        "# do the above in a for loop many times with a lot of images and your vision transformer will learn\n",
        "\n",
        "torch.save(v.state_dict(), './trained-vit.pt')"
      ],
      "metadata": {
        "id": "1IVJxUnuM0G1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from vit_pytorch import ViT, MAE\n",
        "\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "mae = MAE(\n",
        "    encoder = v,\n",
        "    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
        "    decoder_dim = 512,      # paper showed good results with just 512\n",
        "    decoder_depth = 6       # anywhere from 1 to 8\n",
        ")\n",
        "\n",
        "images = torch.randn(8, 3, 256, 256)\n",
        "\n",
        "loss = mae(images)\n",
        "loss.backward()\n",
        "\n",
        "# that's all!\n",
        "# do the above in a for loop many times with a lot of images and your vision transformer will learn\n",
        "\n",
        "# save your improved vision transformer\n",
        "torch.save(v.state_dict(), './trained-vit.pt')"
      ],
      "metadata": {
        "id": "eFp3yhpgM4mF"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}