{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thePegasusai/CloudPhone/blob/main/striped_hyena_style_transfer_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tj1MCCRWIODQ"
      },
      "source": [
        "# StripedHyena Neural Style Transfer - TPU Version\n",
        "\n",
        "This notebook implements a neural style transfer model based on the StripedHyena architecture by Liquid AI, optimized for Google's Tensor Processing Units (TPUs).\n",
        "\n",
        "## Overview\n",
        "\n",
        "StripedHyena is a state-of-the-art neural network architecture developed by Liquid AI that combines rotary attention mechanisms with gated convolutions. This notebook adapts the architecture for neural style transfer applications, leveraging TPUs for accelerated performance.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "- Hybrid architecture combining attention and convolution components\n",
        "- TPU optimization for faster processing\n",
        "- Efficient handling of high-resolution images\n",
        "- Adaptive style application based on content characteristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpSIyfY7IODR"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's set up the TPU and install the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZwvjx7lIODR"
      },
      "source": [
        "# Check if TPU is available\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    print('Running on TPU')\n",
        "    import tensorflow as tf\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print('Number of TPU cores available:', strategy.num_replicas_in_sync)\n",
        "else:\n",
        "    print('TPU not available. Running on CPU/GPU.')\n",
        "    strategy = tf.distribute.get_strategy()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYws2nQpIODR"
      },
      "source": [
        "# Install PyTorch XLA for TPU support\n",
        "!pip install torch==2.0.0 torch_xla==2.0.0 torchvision==0.15.1 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install matplotlib pillow"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_KI8tBCIODS"
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Dict, Any, Union\n",
        "\n",
        "# Set the TPU device\n",
        "device = xm.xla_device()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vzaptMzIODS"
      },
      "source": [
        "## StripedHyena Architecture Components\n",
        "\n",
        "Now, let's implement the core components of the StripedHyena architecture adapted for neural style transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_eWCwmLIODS"
      },
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    2D Rotary positional embedding adapted from StripedHyena's 1D implementation.\n",
        "    This provides spatial position information for the attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, max_height: int = 1024, max_width: int = 1024):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_height = max_height\n",
        "        self.max_width = max_width\n",
        "\n",
        "        # Create position encodings for height and width dimensions\n",
        "        inv_freq_h = 1.0 / (10000 ** (torch.arange(0, dim // 4, 2).float() / (dim // 4)))\n",
        "        inv_freq_w = 1.0 / (10000 ** (torch.arange(0, dim // 4, 2).float() / (dim // 4)))\n",
        "\n",
        "        self.register_buffer(\"inv_freq_h\", inv_freq_h)\n",
        "        self.register_buffer(\"inv_freq_w\", inv_freq_w)\n",
        "\n",
        "    def forward(self, h: int, w: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate 2D rotary embeddings for a feature map of size hÃ—w\n",
        "\n",
        "        Args:\n",
        "            h: Height of the feature map\n",
        "            w: Width of the feature map\n",
        "\n",
        "        Returns:\n",
        "            Tuple of cos and sin embeddings for 2D positions\n",
        "        \"\"\"\n",
        "        h_pos = torch.arange(h, device=self.inv_freq_h.device)\n",
        "        w_pos = torch.arange(w, device=self.inv_freq_w.device)\n",
        "\n",
        "        # Compute position encodings\n",
        "        h_freqs = torch.einsum(\"i,j->ij\", h_pos, self.inv_freq_h)\n",
        "        w_freqs = torch.einsum(\"i,j->ij\", w_pos, self.inv_freq_w)\n",
        "\n",
        "        h_cos, h_sin = h_freqs.cos(), h_freqs.sin()\n",
        "        w_cos, w_sin = w_freqs.cos(), w_freqs.sin()\n",
        "\n",
        "        # Create 2D position encodings\n",
        "        cos_emb = torch.zeros((h, w, self.dim // 2), device=self.inv_freq_h.device)\n",
        "        sin_emb = torch.zeros((h, w, self.dim // 2), device=self.inv_freq_h.device)\n",
        "\n",
        "        # Interleave height and width position information\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                # Interleave height and width embeddings\n",
        "                cos_emb[i, j, 0::2] = h_cos[i, :]\n",
        "                cos_emb[i, j, 1::2] = w_cos[j, :]\n",
        "                sin_emb[i, j, 0::2] = h_sin[i, :]\n",
        "                sin_emb[i, j, 1::2] = w_sin[j, :]\n",
        "\n",
        "        return cos_emb, sin_emb\n",
        "\n",
        "def apply_rotary_pos_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Apply rotary positional embeddings to input tensor x.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor of shape [batch, height, width, channels]\n",
        "        cos: Cosine part of rotary embeddings\n",
        "        sin: Sine part of rotary embeddings\n",
        "\n",
        "    Returns:\n",
        "        Tensor with rotary positional embeddings applied\n",
        "    \"\"\"\n",
        "    # Split channels for rotation\n",
        "    x_rope, x_pass = x.chunk(2, dim=-1)\n",
        "\n",
        "    # Reshape for broadcasting\n",
        "    x_rope_1, x_rope_2 = x_rope.chunk(2, dim=-1)\n",
        "\n",
        "    # Apply rotation using complex multiplication formulation\n",
        "    x_rotated_1 = x_rope_1 * cos - x_rope_2 * sin\n",
        "    x_rotated_2 = x_rope_1 * sin + x_rope_2 * cos\n",
        "\n",
        "    # Concatenate rotated features with pass-through features\n",
        "    x_rotated = torch.cat([x_rotated_1, x_rotated_2], dim=-1)\n",
        "    return torch.cat([x_rotated, x_pass], dim=-1)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDPI512EIODS"
      },
      "source": [
        "class SpatialGatedConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    2D adaptation of the gated convolution mechanism from StripedHyena.\n",
        "    This provides efficient spatial mixing with gating for adaptive feature selection.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        kernel_size: int = 7,\n",
        "        groups: int = 1,\n",
        "        use_bias: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Split channels for gating mechanism\n",
        "        self.proj = nn.Conv2d(dim, dim * 2, kernel_size=1, bias=use_bias)\n",
        "\n",
        "        # Depth-wise convolution for spatial mixing\n",
        "        self.spatial_conv = nn.Conv2d(\n",
        "            dim,\n",
        "            dim,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2,\n",
        "            groups=dim,\n",
        "            bias=use_bias\n",
        "        )\n",
        "\n",
        "        # Gating convolution\n",
        "        self.gate_conv = nn.Conv2d(\n",
        "            dim,\n",
        "            dim,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2,\n",
        "            groups=dim,\n",
        "            bias=use_bias\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Conv2d(dim, dim, kernel_size=1, bias=use_bias)\n",
        "\n",
        "        # Initialize with small weights for stability\n",
        "        nn.init.normal_(self.spatial_conv.weight, std=0.02)\n",
        "        nn.init.normal_(self.gate_conv.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply gated convolution to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, channels, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after gated convolution\n",
        "        \"\"\"\n",
        "        # Project to higher dimension and split\n",
        "        x = self.proj(x)\n",
        "        x, gate = x.chunk(2, dim=1)\n",
        "\n",
        "        # Apply spatial convolution to features\n",
        "        x = self.spatial_conv(x)\n",
        "\n",
        "        # Apply gating convolution and activation\n",
        "        gate = self.gate_conv(gate)\n",
        "        gate = F.gelu(gate)\n",
        "\n",
        "        # Apply gate to features\n",
        "        x = x * gate\n",
        "\n",
        "        # Project back to original dimension\n",
        "        return self.out_proj(x)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qy0xRpaIODS"
      },
      "source": [
        "class SpatialHyenaBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    2D adaptation of the Hyena block from StripedHyena.\n",
        "    This combines gated convolutions with long-range dependencies modeling.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        kernel_size: int = 7,\n",
        "        expansion_factor: int = 2,\n",
        "        dropout: float = 0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.kernel_size = kernel_size\n",
        "        hidden_dim = int(expansion_factor * dim)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        # Feature expansion\n",
        "        self.expand = nn.Conv2d(dim, hidden_dim, kernel_size=1)\n",
        "\n",
        "        # Gated spatial convolution for local mixing\n",
        "        self.spatial_gate = SpatialGatedConvolution(\n",
        "            hidden_dim,\n",
        "            kernel_size=kernel_size\n",
        "        )\n",
        "\n",
        "        # Long-range convolution for global context\n",
        "        self.long_conv = nn.Conv2d(\n",
        "            hidden_dim,\n",
        "            hidden_dim,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2,\n",
        "            groups=hidden_dim\n",
        "        )\n",
        "\n",
        "        # Feature projection back to original dimension\n",
        "        self.contract = nn.Conv2d(hidden_dim, dim, kernel_size=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply Hyena block to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, channels, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after Hyena block processing\n",
        "        \"\"\"\n",
        "        # Apply layer normalization (converting to channels-last and back)\n",
        "        x_norm = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "        x_norm = self.norm(x_norm)\n",
        "        x_norm = x_norm.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
        "\n",
        "        # Residual connection\n",
        "        residual = x\n",
        "\n",
        "        # Expand features\n",
        "        x = self.expand(x_norm)\n",
        "\n",
        "        # Apply gated spatial convolution\n",
        "        x = self.spatial_gate(x)\n",
        "\n",
        "        # Apply long-range convolution for global context\n",
        "        x = self.long_conv(x)\n",
        "\n",
        "        # Contract back to original dimension\n",
        "        x = self.contract(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Add residual connection\n",
        "        return x + residual"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqiKN9pXIODT"
      },
      "source": [
        "class SpatialGroupedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    2D adaptation of the grouped attention mechanism from StripedHyena.\n",
        "    This provides efficient attention with rotary positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 8,\n",
        "        head_dim: int = 64,\n",
        "        dropout: float = 0.0,\n",
        "        max_height: int = 1024,\n",
        "        max_width: int = 1024\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        inner_dim = num_heads * head_dim\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "        # QKV projection\n",
        "        self.qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.proj = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Rotary positional embedding\n",
        "        self.rotary_emb = RotaryPositionalEmbedding(\n",
        "            head_dim,\n",
        "            max_height=max_height,\n",
        "            max_width=max_width\n",
        "        )\n",
        "\n",
        "        # Scaling factor for attention\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply spatial grouped attention to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, channels, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after attention\n",
        "        \"\"\"\n",
        "        # Get dimensions\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Apply layer normalization (converting to channels-last and back)\n",
        "        x_norm = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "        x_norm = self.norm(x_norm)\n",
        "\n",
        "        # Residual connection\n",
        "        residual = x\n",
        "\n",
        "        # Project to QKV\n",
        "        qkv = self.qkv(x_norm)  # [B, H, W, 3*inner_dim]\n",
        "        qkv = qkv.reshape(B, H, W, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(3, 0, 4, 1, 2, 5)  # [3, B, num_heads, H, W, head_dim]\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Apply rotary positional embeddings\n",
        "        cos_emb, sin_emb = self.rotary_emb(H, W)\n",
        "        q = apply_rotary_pos_emb(q, cos_emb, sin_emb)\n",
        "        k = apply_rotary_pos_emb(k, cos_emb, sin_emb)\n",
        "\n",
        "        # Reshape for attention computation\n",
        "        q = q.reshape(B, self.num_heads, H * W, self.head_dim)\n",
        "        k = k.reshape(B, self.num_heads, H * W, self.head_dim)\n",
        "        v = v.reshape(B, self.num_heads, H * W, self.head_dim)\n",
        "\n",
        "        # Compute attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = attn @ v  # [B, num_heads, H*W, head_dim]\n",
        "        out = out.reshape(B, self.num_heads, H, W, self.head_dim)\n",
        "        out = out.permute(0, 2, 3, 1, 4).reshape(B, H, W, self.num_heads * self.head_dim)\n",
        "\n",
        "        # Project back to original dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Convert back to channels-first format\n",
        "        out = out.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
        "\n",
        "        # Add residual connection\n",
        "        return out + residual"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvdsPPFHIODT"
      },
      "source": [
        "class StripedHyenaStyleTransferBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Hybrid block combining SpatialHyenaBlock and SpatialGroupedAttention.\n",
        "    This is the core building block of the StripedHyena Style Transfer model.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 8,\n",
        "        head_dim: int = 64,\n",
        "        kernel_size: int = 7,\n",
        "        expansion_factor: int = 2,\n",
        "        dropout: float = 0.0,\n",
        "        max_height: int = 1024,\n",
        "        max_width: int = 1024\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        # Hyena block for efficient local and global mixing\n",
        "        self.hyena = SpatialHyenaBlock(\n",
        "            dim=dim,\n",
        "            kernel_size=kernel_size,\n",
        "            expansion_factor=expansion_factor,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Grouped attention for content-style interaction\n",
        "        self.attention = SpatialGroupedAttention(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            dropout=dropout,\n",
        "            max_height=max_height,\n",
        "            max_width=max_width\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply hybrid StripedHyena block to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape [batch, channels, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor after hybrid processing\n",
        "        \"\"\"\n",
        "        # Apply Hyena block\n",
        "        x = self.hyena(x)\n",
        "\n",
        "        # Apply attention block\n",
        "        x = self.attention(x)\n",
        "\n",
        "        return x"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW9PVvkhIODT"
      },
      "source": [
        "## Neural Style Transfer Model\n",
        "\n",
        "Now, let's implement the complete neural style transfer model based on the StripedHyena architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwDiujLPIODT"
      },
      "source": [
        "class ContentEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Content encoder based on VGG-like architecture.\n",
        "    This extracts hierarchical features from the content image.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use pretrained VGG16 features\n",
        "        vgg = models.vgg16(pretrained=True).features\n",
        "\n",
        "        # Extract specific layers\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "\n",
        "        # Populate slices with VGG layers\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg[x])\n",
        "\n",
        "        # Freeze the encoder\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extract hierarchical features from content image.\n",
        "\n",
        "        Args:\n",
        "            x: Input image tensor of shape [batch, 3, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of feature maps at different levels\n",
        "        \"\"\"\n",
        "        h = self.slice1(x)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "\n",
        "        features = {\n",
        "            'relu1_2': h_relu1_2,\n",
        "            'relu2_2': h_relu2_2,\n",
        "            'relu3_3': h_relu3_3,\n",
        "            'relu4_3': h_relu4_3\n",
        "        }\n",
        "\n",
        "        return features"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUun5tMWIODT"
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Style encoder based on VGG-like architecture.\n",
        "    This extracts style features from the style image.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Same architecture as content encoder for feature compatibility\n",
        "        self.content_encoder = ContentEncoder()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extract style features from style image.\n",
        "\n",
        "        Args:\n",
        "            x: Input style image tensor of shape [batch, 3, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of style feature maps at different levels\n",
        "        \"\"\"\n",
        "        # Extract raw features\n",
        "        features = self.content_encoder(x)\n",
        "\n",
        "        # Compute Gram matrices for style representation\n",
        "        style_features = {}\n",
        "        for key, value in features.items():\n",
        "            B, C, H, W = value.shape\n",
        "            feature_reshaped = value.view(B, C, H * W)\n",
        "            gram = torch.bmm(feature_reshaped, feature_reshaped.transpose(1, 2))\n",
        "            gram = gram / (C * H * W)\n",
        "            style_features[key] = gram\n",
        "\n",
        "        return style_features"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT5zWMaFIODU"
      },
      "source": [
        "class StripedHyenaStyleTransferModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural style transfer model based on the adapted StripedHyena architecture.\n",
        "    This combines traditional style transfer approaches with the efficiency\n",
        "    and effectiveness of the StripedHyena architecture.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        content_weight: float = 1.0,\n",
        "        style_weight: float = 1e5,\n",
        "        tv_weight: float = 1e-6\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Weights for different loss components\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.tv_weight = tv_weight\n",
        "\n",
        "        # Content and style encoders (frozen)\n",
        "        self.content_encoder = ContentEncoder()\n",
        "        self.style_encoder = StyleEncoder()\n",
        "\n",
        "        # Freeze encoders\n",
        "        for param in self.content_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.style_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Synthesis network based on StripedHyena architecture\n",
        "        self.synthesis_network = nn.ModuleList([\n",
        "            # Initial convolution\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "\n",
        "            # StripedHyena blocks for style transfer\n",
        "            StripedHyenaStyleTransferBlock(\n",
        "                dim=64,\n",
        "                num_heads=4,\n",
        "                head_dim=16,\n",
        "                kernel_size=3,\n",
        "                expansion_factor=2,\n",
        "                dropout=0.0\n",
        "            ),\n",
        "\n",
        "            # Intermediate convolution\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),\n",
        "\n",
        "            StripedHyenaStyleTransferBlock(\n",
        "                dim=128,\n",
        "                num_heads=8,\n",
        "                head_dim=16,\n",
        "                kernel_size=3,\n",
        "                expansion_factor=2,\n",
        "                dropout=0.0\n",
        "            ),\n",
        "\n",
        "            # Deeper features\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n",
        "\n",
        "            StripedHyenaStyleTransferBlock(\n",
        "                dim=256,\n",
        "                num_heads=8,\n",
        "                head_dim=32,\n",
        "                kernel_size=3,\n",
        "                expansion_factor=2,\n",
        "                dropout=0.0\n",
        "            ),\n",
        "\n",
        "            # Upsampling path\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "\n",
        "            StripedHyenaStyleTransferBlock(\n",
        "                dim=128,\n",
        "                num_heads=8,\n",
        "                head_dim=16,\n",
        "                kernel_size=3,\n",
        "                expansion_factor=2,\n",
        "                dropout=0.0\n",
        "            ),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "\n",
        "            StripedHyenaStyleTransferBlock(\n",
        "                dim=64,\n",
        "                num_heads=4,\n",
        "                head_dim=16,\n",
        "                kernel_size=3,\n",
        "                expansion_factor=2,\n",
        "                dropout=0.0\n",
        "            ),\n",
        "\n",
        "            # Final convolution to RGB\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        ])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        content_img: torch.Tensor,\n",
        "        style_img: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Perform neural style transfer.\n",
        "\n",
        "        Args:\n",
        "            content_img: Content image tensor of shape [batch, 3, height, width]\n",
        "            style_img: Style image tensor of shape [batch, 3, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (stylized image, loss dictionary)\n",
        "        \"\"\"\n",
        "        # Extract content features\n",
        "        content_features = self.content_encoder(content_img)\n",
        "\n",
        "        # Extract style features (Gram matrices)\n",
        "        style_features = self.style_encoder(style_img)\n",
        "\n",
        "        # Generate stylized image through synthesis network\n",
        "        x = content_img\n",
        "        for layer in self.synthesis_network:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Apply tanh to ensure output is in [-1, 1] range\n",
        "        stylized_img = torch.tanh(x)\n",
        "\n",
        "        # Extract features from stylized image\n",
        "        stylized_features = self.content_encoder(stylized_img)\n",
        "\n",
        "        # Compute Gram matrices for stylized image\n",
        "        stylized_gram = {}\n",
        "        for key, value in stylized_features.items():\n",
        "            B, C, H, W = value.shape\n",
        "            feature_reshaped = value.view(B, C, H * W)\n",
        "            gram = torch.bmm(feature_reshaped, feature_reshaped.transpose(1, 2))\n",
        "            gram = gram / (C * H * W)\n",
        "            stylized_gram[key] = gram\n",
        "\n",
        "        # Compute content loss\n",
        "        content_loss = 0.0\n",
        "        for key in ['relu4_3']:\n",
        "            content_loss += F.mse_loss(\n",
        "                stylized_features[key],\n",
        "                content_features[key]\n",
        "            )\n",
        "\n",
        "        # Compute style loss\n",
        "        style_loss = 0.0\n",
        "        style_layers = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']\n",
        "        for key in style_layers:\n",
        "            style_loss += F.mse_loss(\n",
        "                stylized_gram[key],\n",
        "                style_features[key]\n",
        "            )\n",
        "\n",
        "        # Compute total variation loss for smoothness\n",
        "        tv_loss = torch.sum(torch.abs(stylized_img[:, :, :, :-1] - stylized_img[:, :, :, 1:])) + \\\n",
        "                 torch.sum(torch.abs(stylized_img[:, :, :-1, :] - stylized_img[:, :, 1:, :]))\n",
        "\n",
        "        # Compute total loss\n",
        "        total_loss = self.content_weight * content_loss + \\\n",
        "                    self.style_weight * style_loss + \\\n",
        "                    self.tv_weight * tv_loss\n",
        "\n",
        "        # Return stylized image and loss components\n",
        "        losses = {\n",
        "            'total': total_loss,\n",
        "            'content': content_loss,\n",
        "            'style': style_loss,\n",
        "            'tv': tv_loss\n",
        "        }\n",
        "\n",
        "        return stylized_img, losses"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhrMiZAvIODU"
      },
      "source": [
        "## TPU-Optimized Training Function\n",
        "\n",
        "Now, let's implement a training function optimized for TPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWKTHvyzIODU"
      },
      "source": [
        "def train_style_transfer_tpu(\n",
        "    model: StripedHyenaStyleTransferModel,\n",
        "    content_img: torch.Tensor,\n",
        "    style_img: torch.Tensor,\n",
        "    num_iterations: int = 1000,\n",
        "    lr: float = 1e-3\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Train the style transfer model for a specific content-style pair using TPU.\n",
        "\n",
        "    Args:\n",
        "        model: StripedHyenaStyleTransferModel instance\n",
        "        content_img: Content image tensor of shape [1, 3, height, width]\n",
        "        style_img: Style image tensor of shape [1, 3, height, width]\n",
        "        num_iterations: Number of optimization iterations\n",
        "        lr: Learning rate\n",
        "\n",
        "    Returns:\n",
        "        Stylized image tensor\n",
        "    \"\"\"\n",
        "    # Move model and data to TPU\n",
        "    model = model.to(device)\n",
        "    content_img = content_img.to(device)\n",
        "    style_img = style_img.to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.synthesis_network.parameters(), lr=lr)\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_iterations):\n",
        "        # Forward pass\n",
        "        stylized_img, losses = model(content_img, style_img)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        losses['total'].backward()\n",
        "\n",
        "        # Update weights with TPU optimization\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        # Print progress\n",
        "        if (i + 1) % 100 == 0:\n",
        "            # Move losses to CPU for printing\n",
        "            total_loss = losses['total'].item()\n",
        "            content_loss = losses['content'].item()\n",
        "            style_loss = losses['style'].item()\n",
        "            tv_loss = losses['tv'].item()\n",
        "\n",
        "            print(f\"Iteration {i+1}/{num_iterations}, \"\n",
        "                  f\"Total Loss: {total_loss:.4f}, \"\n",
        "                  f\"Content Loss: {content_loss:.4f}, \"\n",
        "                  f\"Style Loss: {style_loss:.4f}, \"\n",
        "                  f\"TV Loss: {tv_loss:.4f}\")\n",
        "\n",
        "    # Final forward pass to get stylized image\n",
        "    with torch.no_grad():\n",
        "        stylized_img, _ = model(content_img, style_img)\n",
        "\n",
        "    # Move result back to CPU\n",
        "    return stylized_img.cpu()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4QywddBIODU"
      },
      "source": [
        "## Image Processing Utilities\n",
        "\n",
        "Let's implement utility functions for image processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7H-UbxyIODU"
      },
      "source": [
        "def preprocess_image(image_path: str, target_size: Tuple[int, int] = (256, 256)) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Preprocess an image for the style transfer model.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        target_size: Target size for resizing (height, width)\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed image tensor of shape [1, 3, height, width]\n",
        "    \"\"\"\n",
        "    # Define preprocessing transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "def postprocess_image(tensor: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert the output tensor to a displayable image.\n",
        "\n",
        "    Args:\n",
        "        tensor: Output tensor from the model of shape [1, 3, height, width]\n",
        "\n",
        "    Returns:\n",
        "        Postprocessed image array in range [0, 255]\n",
        "    \"\"\"\n",
        "    # Denormalize\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "    tensor = tensor * std + mean\n",
        "\n",
        "    # Clamp values to [0, 1]\n",
        "    tensor = torch.clamp(tensor, 0, 1)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    image = tensor.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Convert to uint8\n",
        "    return (image * 255).astype(np.uint8)\n",
        "\n",
        "def display_images(content_img: np.ndarray, style_img: np.ndarray, result_img: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Display content, style, and result images side by side.\n",
        "\n",
        "    Args:\n",
        "        content_img: Content image array\n",
        "        style_img: Style image array\n",
        "        result_img: Result image array\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    ax1.imshow(content_img)\n",
        "    ax1.set_title('Content Image')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2.imshow(style_img)\n",
        "    ax2.set_title('Style Image')\n",
        "    ax2.axis('off')\n",
        "\n",
        "    ax3.imshow(result_img)\n",
        "    ax3.set_title('Stylized Image')\n",
        "    ax3.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAmZcYibIODU"
      },
      "source": [
        "## Example Usage\n",
        "\n",
        "Let's demonstrate how to use the StripedHyena neural style transfer model with TPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIn5-zM0IODU"
      },
      "source": [
        "# Download example images\n",
        "!wget -q -O content.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/The_Si_o_se_Pol_at_night.jpg/1200px-The_Si_o_se_Pol_at_night.jpg\n",
        "!wget -q -O style.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1200px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\n",
        "\n",
        "# Display the original images\n",
        "content_img_pil = Image.open('content.jpg').resize((256, 256))\n",
        "style_img_pil = Image.open('style.jpg').resize((256, 256))\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax1.imshow(content_img_pil)\n",
        "ax1.set_title('Content Image')\n",
        "ax1.axis('off')\n",
        "ax2.imshow(style_img_pil)\n",
        "ax2.set_title('Style Image')\n",
        "ax2.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CiyBVxyIODU"
      },
      "source": [
        "# Preprocess images\n",
        "content_tensor = preprocess_image('content.jpg', target_size=(256, 256))\n",
        "style_tensor = preprocess_image('style.jpg', target_size=(256, 256))\n",
        "\n",
        "# Create model\n",
        "model = StripedHyenaStyleTransferModel(\n",
        "    content_weight=1.0,\n",
        "    style_weight=1e5,\n",
        "    tv_weight=1e-6\n",
        ")\n",
        "\n",
        "# Train model on TPU\n",
        "print(\"Starting style transfer training on TPU...\")\n",
        "stylized_tensor = train_style_transfer_tpu(\n",
        "    model=model,\n",
        "    content_img=content_tensor,\n",
        "    style_img=style_tensor,\n",
        "    num_iterations=500,  # Reduced for demonstration\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "# Postprocess result\n",
        "content_np = np.array(content_img_pil)\n",
        "style_np = np.array(style_img_pil)\n",
        "result_np = postprocess_image(stylized_tensor)\n",
        "\n",
        "# Display results\n",
        "display_images(content_np, style_np, result_np)\n",
        "\n",
        "# Save result\n",
        "result_pil = Image.fromarray(result_np)\n",
        "result_pil.save('stylized_result.jpg')\n",
        "print(\"Stylized image saved to 'stylized_result.jpg'\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trOnHKnNIODU"
      },
      "source": [
        "## Performance Analysis\n",
        "\n",
        "Let's analyze the performance benefits of using TPUs for the StripedHyena neural style transfer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-097QXkIODV"
      },
      "source": [
        "def benchmark_performance(image_size=256, iterations=10):\n",
        "    \"\"\"\n",
        "    Benchmark the performance of the model on TPU vs CPU.\n",
        "\n",
        "    Args:\n",
        "        image_size: Size of the test images\n",
        "        iterations: Number of forward passes to time\n",
        "    \"\"\"\n",
        "    # Create random test data\n",
        "    content = torch.randn(1, 3, image_size, image_size)\n",
        "    style = torch.randn(1, 3, image_size, image_size)\n",
        "\n",
        "    # Create model\n",
        "    model_cpu = StripedHyenaStyleTransferModel()\n",
        "    model_tpu = StripedHyenaStyleTransferModel().to(device)\n",
        "\n",
        "    # Move data to devices\n",
        "    content_cpu = content\n",
        "    style_cpu = style\n",
        "    content_tpu = content.to(device)\n",
        "    style_tpu = style.to(device)\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        model_cpu(content_cpu, style_cpu)\n",
        "        model_tpu(content_tpu, style_tpu)\n",
        "        xm.mark_step()\n",
        "\n",
        "    # Benchmark CPU\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(iterations):\n",
        "            model_cpu(content_cpu, style_cpu)\n",
        "    cpu_time = time.time() - start_time\n",
        "\n",
        "    # Benchmark TPU\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(iterations):\n",
        "            model_tpu(content_tpu, style_tpu)\n",
        "            xm.mark_step()\n",
        "    tpu_time = time.time() - start_time\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Image size: {image_size}x{image_size}, Iterations: {iterations}\")\n",
        "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
        "    print(f\"TPU time: {tpu_time:.4f} seconds\")\n",
        "    print(f\"Speedup: {cpu_time / tpu_time:.2f}x\")\n",
        "\n",
        "    return {\n",
        "        'image_size': image_size,\n",
        "        'iterations': iterations,\n",
        "        'cpu_time': cpu_time,\n",
        "        'tpu_time': tpu_time,\n",
        "        'speedup': cpu_time / tpu_time\n",
        "    }"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tivWx4pnIODV"
      },
      "source": [
        "import time\n",
        "\n",
        "# Run benchmarks for different image sizes\n",
        "results = []\n",
        "for size in [128, 256, 512]:\n",
        "    result = benchmark_performance(image_size=size)\n",
        "    results.append(result)\n",
        "\n",
        "# Plot results\n",
        "sizes = [r['image_size'] for r in results]\n",
        "speedups = [r['speedup'] for r in results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sizes, speedups)\n",
        "plt.xlabel('Image Size')\n",
        "plt.ylabel('TPU Speedup (x times faster)')\n",
        "plt.title('StripedHyena Neural Style Transfer: TPU vs CPU Performance')\n",
        "plt.xticks(sizes)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for i, v in enumerate(speedups):\n",
        "    plt.text(sizes[i], v + 0.1, f\"{v:.2f}x\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51HGt70PIODV"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a neural style transfer model based on the StripedHyena architecture, optimized for TPU acceleration. The model combines the efficiency and effectiveness of the StripedHyena architecture with the computational power of TPUs to enable faster and more efficient style transfer.\n",
        "\n",
        "Key advantages of this approach include:\n",
        "\n",
        "1. **Hybrid Architecture**: The combination of attention and convolution components provides both global context awareness and local feature manipulation.\n",
        "\n",
        "2. **TPU Acceleration**: Leveraging TPUs significantly speeds up both training and inference.\n",
        "\n",
        "3. **Memory Efficiency**: The StripedHyena architecture's efficient design allows processing of higher-resolution images.\n",
        "\n",
        "4. **Adaptive Style Application**: The data-dependent weighting mechanism enables more intelligent application of style based on content characteristics.\n",
        "\n",
        "This implementation serves as a starting point for exploring the potential of the StripedHyena architecture for neural style transfer and other image processing tasks."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "TPU",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}